import { NextResponse } from 'next/server'

// Cache simples para respostas (em produ√ß√£o, use Redis ou similar)
const responseCache = new Map()

// Fun√ß√£o para gerar hash da requisi√ß√£o
function generateRequestHash(message, imageSize) {
  return Buffer.from(message + (imageSize || '')).toString('base64').slice(0, 32)
}

// Fun√ß√£o para detectar solicita√ß√µes de gera√ß√£o de imagem
function isImageGenerationRequest(message) {
  const imageKeywords = [
    'crie uma imagem', 'crie a imagem', 'criar imagem', 'gerar imagem', 'gere uma imagem',
    'fa√ßa uma imagem', 'fa√ßa a imagem', 'desenhe', 'ilustre', 'create image', 'generate image',
    'crie um desenho', 'fa√ßa um desenho', 'gere um desenho'
  ]
  
  const lowerMessage = message.toLowerCase()
  return imageKeywords.some(keyword => lowerMessage.includes(keyword))
}

// Fun√ß√£o para gerar imagem usando Pollinations AI (gratuita)
async function generateImage(prompt) {
  try {
    console.log('=== INICIANDO GERA√á√ÉO DE IMAGEM ===')
    console.log('Prompt original:', prompt)
    
    // Traduzir e otimizar prompt
    let optimizedPrompt = prompt
      .replace(/crie uma imagem de|crie a imagem de|criar imagem de|gerar imagem de|gere uma imagem de|fa√ßa uma imagem de|fa√ßa a imagem de|desenhe|ilustre|crie um desenho de|fa√ßa um desenho de|gere um desenho de/gi, '')
      .trim()
    
    console.log('Prompt ap√≥s limpeza:', optimizedPrompt)
    
    // Se o prompt estiver vazio, usar um padr√£o
    if (!optimizedPrompt) {
      optimizedPrompt = "beautiful landscape"
      console.log('Prompt vazio, usando padr√£o:', optimizedPrompt)
    }
    
    // URL da API Pollinations (gratuita)
    const externalImageUrl = `https://image.pollinations.ai/prompt/${encodeURIComponent(optimizedPrompt)}?width=512&height=512&seed=${Date.now()}`
    
    console.log('URL da imagem externa:', externalImageUrl)
    
    return {
      success: true,
      imageUrl: externalImageUrl,
      prompt: optimizedPrompt
    }
    
  } catch (error) {
    console.error('ERRO GERAL na gera√ß√£o de imagem:', error)
    return {
      success: false,
      error: error.message || 'Falha na gera√ß√£o da imagem. Tente novamente.'
    }
  }
}

export async function POST(request) {
  try {
    const { message, image, conversationHistory } = await request.json()
    
    console.log('=== PROCESSANDO MENSAGEM ===')
    console.log('Mensagem recebida:', message)
    console.log('Teste de gera√ß√£o de imagem:', isImageGenerationRequest(message))
    
    // Verificar se √© uma solicita√ß√£o de gera√ß√£o de imagem
    if (isImageGenerationRequest(message)) {
      console.log('‚úÖ Detectada solicita√ß√£o de gera√ß√£o de imagem')
      
      const imageResult = await generateImage(message)
      
      if (imageResult.success) {
        const responseData = {
          response: `üé® **Imagem Gerada com Sucesso!**\n\n**Prompt usado:** ${imageResult.prompt}\n\n*Imagem criada usando Pollinations AI - uma ferramenta gratuita de gera√ß√£o de imagens por IA.*`,
          imageUrl: imageResult.imageUrl,
          isImageGeneration: true
        }
        
        return NextResponse.json(responseData)
      } else {
        return NextResponse.json({
          response: `‚ùå **Erro ao gerar imagem**\n\nN√£o foi poss√≠vel criar a imagem solicitada. Erro: ${imageResult.error}\n\nTente reformular sua solicita√ß√£o ou tente novamente em alguns minutos.`,
          isImageGeneration: true
        })
      }
    }
    
    // Gerar hash para cache
    const imageSize = image ? image.length : null
    const requestHash = generateRequestHash(message, imageSize)
    
    // Verificar cache primeiro
    if (responseCache.has(requestHash)) {
      console.log('Resposta encontrada no cache')
      return NextResponse.json(responseCache.get(requestHash))
    }
    
    // Verificar se as configura√ß√µes do Azure OpenAI est√£o definidas
    const apiKey = process.env.AZURE_OPENAI_API_KEY
    const endpoint = process.env.AZURE_OPENAI_ENDPOINT
    const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || 'o4-mini'
    const apiVersion = process.env.AZURE_OPENAI_API_VERSION || '2024-12-01-preview'
    
    if (!apiKey || !endpoint) {
      console.error('Configura√ß√µes do Azure OpenAI n√£o encontradas')
      return NextResponse.json(
        { error: 'Servi√ßo temporariamente indispon√≠vel. Tente novamente mais tarde.' },
        { status: 503 }
      )
    }

    // Preparar mensagens para Azure OpenAI
    let messages = [
      {
        role: "system",
        content: "Voc√™ √© um assistente inteligente e √∫til. Responda de forma clara, precisa e educativa. Se uma imagem for enviada, analise-a detalhadamente e forne√ßa informa√ß√µes relevantes."
      }
    ]

    // Adicionar hist√≥rico da conversa se existir
    if (conversationHistory && Array.isArray(conversationHistory)) {
      messages.push(...conversationHistory.slice(-10)) // Limitar a 10 mensagens recentes
    }

    // Preparar a mensagem atual
    let currentMessage = {
      role: "user",
      content: message
    }

    // Se h√° uma imagem, incluir na mensagem
    if (image) {
      try {
        console.log('Processando imagem para an√°lise no Azure OpenAI...')
        
        currentMessage = {
          role: "user",
          content: [
            {
              type: "text",
              text: message
            },
            {
              type: "image_url",
              image_url: {
                url: image,
                detail: "high"
              }
            }
          ]
        }
        
        console.log('Imagem adicionada √† requisi√ß√£o do Azure OpenAI')
      } catch (imageError) {
        console.error('Erro ao processar imagem:', imageError)
        return NextResponse.json(
          { error: 'Erro ao processar a imagem enviada' },
          { status: 400 }
        )
      }
    }

    messages.push(currentMessage)

    console.log('Enviando requisi√ß√£o para Azure OpenAI...')
    console.log('Deployment:', deploymentName)
    console.log('Tem imagem:', !!image)
    console.log('Tamanho da mensagem:', message.length)
    
    // Configurar a requisi√ß√£o para Azure OpenAI usando fetch
    const azureUrl = `${endpoint}openai/deployments/${deploymentName}/chat/completions?api-version=${apiVersion}`
    
    const requestBody = {
      messages: messages,
      model: deploymentName, // Baseado na documenta√ß√£o Azure
      max_completion_tokens: 4000,
      temperature: 1 // o4-mini s√≥ aceita temperature = 1 e n√£o aceita top_p
    }

    // Fazer a requisi√ß√£o para o Azure OpenAI
    const response = await fetch(azureUrl, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'api-key': apiKey
      },
      body: JSON.stringify(requestBody)
    })

    console.log('Status da resposta:', response.status)
    
    // Log detalhado do erro para debugging
    if (!response.ok) {
      const errorText = await response.text()
      console.error(`Erro na API do Azure OpenAI (${response.status}):`, errorText)
      
      let errorMessage = 'Desculpe, n√£o consegui processar sua solicita√ß√£o no momento. Tente novamente.'
      
      if (response.status === 429) {
        errorMessage = 'Muitas solicita√ß√µes foram feitas. Aguarde alguns minutos e tente novamente.'
      } else if (response.status >= 500) {
        errorMessage = 'Servi√ßo temporariamente indispon√≠vel. Tente novamente em alguns minutos.'
      } else if (response.status === 401) {
        errorMessage = 'Erro de autentica√ß√£o. Verifique as configura√ß√µes da API.'
      } else if (response.status === 404) {
        errorMessage = `Deployment "${deploymentName}" n√£o encontrado. Verifique a configura√ß√£o do Azure OpenAI.`
      }
      
      return NextResponse.json(
        { error: errorMessage },
        { status: response.status }
      )
    }

    const data = await response.json()
    console.log('Resposta recebida do Azure OpenAI')
    
    // Verificar se a resposta tem o formato esperado
    if (!data.choices || data.choices.length === 0) {
      console.error('Resposta inv√°lida do Azure OpenAI:', data)
      return NextResponse.json(
        { error: 'N√£o foi poss√≠vel gerar uma resposta. Tente reformular sua pergunta.' },
        { status: 500 }
      )
    }
    
    // Extrair a resposta
    const aiResponse = data.choices[0]?.message?.content
    
    if (!aiResponse) {
      console.error('Texto de resposta n√£o encontrado:', data.choices[0])
      return NextResponse.json(
        { error: 'N√£o foi poss√≠vel gerar uma resposta. Tente novamente.' },
        { status: 500 }
      )
    }

    const responseData = { response: aiResponse }
    
    // Cache da resposta (com TTL de 1 hora)
    responseCache.set(requestHash, responseData)
    setTimeout(() => responseCache.delete(requestHash), 60 * 60 * 1000)

    return NextResponse.json(responseData)

  } catch (error) {
    console.error('Erro na API do chat:', error)
    
    // Verificar tipos espec√≠ficos de erro
    if (error.code === 'ENOTFOUND' || error.code === 'ECONNREFUSED' || error.code === 'ENETUNREACH') {
      return NextResponse.json(
        { error: 'Servi√ßo de IA temporariamente indispon√≠vel. Verifique sua conex√£o e tente novamente.' },
        { status: 503 }
      )
    }
    
    if (error.name === 'TypeError' && error.message.includes('fetch failed')) {
      return NextResponse.json(
        { error: 'Erro de conectividade. Verifique sua internet e tente novamente.' },
        { status: 503 }
      )
    }
    
    if (error.status === 429) {
      return NextResponse.json(
        { error: 'Muitas solicita√ß√µes foram feitas. Aguarde alguns minutos e tente novamente.' },
        { status: 429 }
      )
    }
    
    if (error.status === 401) {
      return NextResponse.json(
        { error: 'Erro de autentica√ß√£o. Verifique as configura√ß√µes da API.' },
        { status: 401 }
      )
    }
    
    return NextResponse.json(
      { error: 'Erro interno. Tente novamente em alguns instantes.' },
      { status: 500 }
    )
  }
}
